# Real-Time AI Voice Conversations - Feature Location

## ğŸ™ï¸ **Where is this feature in the web app?**

---

## âœ… **WHAT'S BUILT (Frontend UI)**

### **1. Session Lobby Screen** 
**Location**: `/src/app/pages/app/SessionLobby.tsx`  
**Route**: `/app/session-lobby`

**Features:**
- âœ… Start session now or schedule for later
- âœ… Select session duration (15/30/45/60 min)
- âœ… Choose AI avatar (Alex, Sarah, Michael, Emma)
- âœ… Choose voice style (Warm, Calm, Professional, Gentle)
- âœ… Pre-session checklist
- âœ… View upcoming scheduled sessions
- âœ… Beautiful UI with animations

---

### **2. Active Session Screen** 
**Location**: `/src/app/pages/app/ActiveSession.tsx`  
**Route**: `/app/active-session`

**Features:**
- âœ… Live video call interface (FaceTime-style)
- âœ… Avatar video display
- âœ… Microphone toggle (mute/unmute)
- âœ… Camera toggle (on/off)
- âœ… Speaker toggle (sound on/off)
- âœ… Session timer
- âœ… Connection quality indicator
- âœ… Credits/minutes remaining display
- âœ… End session button
- âœ… Permission request flow (mic/camera)
- âœ… AI speaking indicator
- âœ… Safety state monitoring (crisis detection UI)
- âœ… Safety resources panel
- âœ… Beautiful gradient backgrounds
- âœ… Smooth animations

---

### **3. How to Access:**

1. **Login** â†’ `/login`
2. **Dashboard** â†’ `/app/dashboard`
3. **Click "Start Session"** button
4. **Session Lobby** â†’ `/app/session-lobby`
5. **Click "Start Now"**
6. **Active Session** â†’ `/app/active-session` âœ…

---

## âŒ **WHAT'S MISSING (Backend)**

The UI is **100% complete**, but the **actual AI voice functionality** needs backend implementation:

### **Missing Components:**

1. **OpenAI Whisper Integration** âŒ
   - Speech-to-text conversion
   - Real-time transcription
   - **File**: `/backend-starter/ai-service/app/api/v1/endpoints/voice.py` (placeholder only)

2. **ElevenLabs Integration** âŒ
   - Text-to-speech conversion
   - Lifelike voice synthesis
   - Multiple voice options
   - **File**: `/backend-starter/ai-service/app/api/v1/endpoints/voice.py` (placeholder only)

3. **WebSocket Real-Time Communication** âŒ
   - Low-latency audio streaming
   - Bidirectional voice data
   - **File**: `/backend-starter/api-server/src/websocket/index.ts` (placeholder only)

4. **AI Conversation Logic** âŒ
   - GPT-4 conversation management
   - Context preservation
   - Personality system
   - **File**: `/backend-starter/ai-service/app/api/v1/endpoints/conversation.py` (placeholder only)

5. **Session Management** âŒ
   - Create/end sessions
   - Track duration
   - Deduct minutes/credits
   - Save conversation history
   - **File**: `/backend-starter/api-server/src/controllers/sessions.controller.ts` (empty)

---

## ğŸ”§ **CURRENT STATE**

### **What Works Now:**
- âœ… **UI is fully functional** - All buttons, animations, states work
- âœ… **Demo mode** - You can click through the entire flow
- âœ… **Mock data** - Shows sample session data
- âœ… **Local state** - Timer works, buttons toggle
- âœ… **Safety detection UI** - Shows crisis states visually

### **What's Simulated:**
- âš ï¸ **Avatar video** - Static image (not live video)
- âš ï¸ **Voice input** - Mic button works but doesn't capture audio
- âš ï¸ **Voice output** - No actual voice from Ezri
- âš ï¸ **AI responses** - No real AI conversation
- âš ï¸ **Session saving** - No backend persistence

---

## ğŸš€ **HOW TO IMPLEMENT (Backend)**

### **Phase 7 from Task List** - AI Service Integration (8-10 hours)

#### **Step 1: Setup AI Service** (2 hours)
```bash
cd backend-starter/ai-service

# Install dependencies
pip install -r requirements.txt

# Add to .env
OPENAI_API_KEY=sk-...
ELEVENLABS_API_KEY=...

# Start service
uvicorn app.main:app --reload
```

#### **Step 2: Implement Whisper (Speech-to-Text)** (1-2 hours)
**File**: `/backend-starter/ai-service/app/api/v1/endpoints/voice.py`

```python
from openai import OpenAI

@router.post("/transcribe")
async def transcribe_audio(audio: UploadFile):
    """Convert speech to text using Whisper"""
    client = OpenAI(api_key=settings.OPENAI_API_KEY)
    
    audio_file = await audio.read()
    
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        response_format="json"
    )
    
    return {"text": transcript.text}
```

#### **Step 3: Implement ElevenLabs (Text-to-Speech)** (1-2 hours)
**File**: `/backend-starter/ai-service/app/api/v1/endpoints/voice.py`

```python
import requests

@router.post("/synthesize")
async def synthesize_speech(text: str, voice_id: str):
    """Convert text to speech using ElevenLabs"""
    
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
    
    headers = {
        "xi-api-key": settings.ELEVENLABS_API_KEY,
        "Content-Type": "application/json"
    }
    
    data = {
        "text": text,
        "model_id": "eleven_monolingual_v1",
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.75
        }
    }
    
    response = requests.post(url, json=data, headers=headers)
    
    return {"audio_url": response.content}
```

#### **Step 4: Implement WebSocket** (2-3 hours)
**File**: `/backend-starter/api-server/src/websocket/handlers.ts`

```typescript
export const handleVoiceConversation = (socket: Socket) => {
  socket.on('audio-chunk', async (audioData) => {
    // 1. Send audio to AI service for transcription
    const transcript = await aiService.transcribe(audioData);
    
    // 2. Send transcript to GPT-4 for response
    const aiResponse = await aiService.chat(transcript);
    
    // 3. Send response to ElevenLabs for voice
    const audioResponse = await aiService.synthesize(aiResponse);
    
    // 4. Stream audio back to client
    socket.emit('ai-audio', audioResponse);
  });
};
```

#### **Step 5: Connect Frontend to Backend** (1 hour)
**File**: Update `/src/app/pages/app/ActiveSession.tsx`

```typescript
import { io } from 'socket.io-client';

// Connect to WebSocket
const socket = io('http://localhost:3001');

// Send audio to backend
const sendAudioChunk = (audioBlob: Blob) => {
  socket.emit('audio-chunk', audioBlob);
};

// Receive AI audio response
socket.on('ai-audio', (audioData) => {
  const audio = new Audio(audioData);
  audio.play();
});
```

#### **Step 6: Implement Session Management** (1-2 hours)
**File**: `/backend-starter/api-server/src/controllers/sessions.controller.ts`

```typescript
export const createSession = async (req, res) => {
  const { userId, avatarId, duration } = req.body;
  
  const session = await prisma.conversation.create({
    data: {
      userId,
      avatarId,
      sessionId: generateSessionId(),
      status: 'active',
      startedAt: new Date(),
    }
  });
  
  return res.json(session);
};

export const endSession = async (req, res) => {
  const { sessionId } = req.params;
  
  const session = await prisma.conversation.update({
    where: { sessionId },
    data: {
      status: 'completed',
      endedAt: new Date(),
      durationSeconds: calculateDuration(),
    }
  });
  
  // Deduct credits
  await deductUserCredits(session.userId, session.durationSeconds);
  
  return res.json(session);
};
```

---

## ğŸ“‚ **File Locations Summary**

### **Frontend (âœ… COMPLETE)**
```
/src/app/pages/app/
â”œâ”€â”€ SessionLobby.tsx        âœ… Session setup & scheduling
â”œâ”€â”€ ActiveSession.tsx       âœ… Live voice call interface
â””â”€â”€ SessionHistory.tsx      âœ… Past sessions view
```

### **Backend (âŒ NEEDS IMPLEMENTATION)**
```
/backend-starter/ai-service/app/api/v1/endpoints/
â”œâ”€â”€ voice.py                âŒ Whisper + ElevenLabs
â”œâ”€â”€ conversation.py         âŒ GPT-4 chat logic
â””â”€â”€ crisis.py               âŒ Crisis detection

/backend-starter/api-server/src/
â”œâ”€â”€ websocket/
â”‚   â”œâ”€â”€ index.ts            âŒ WebSocket server setup
â”‚   â””â”€â”€ handlers.ts         âŒ Audio streaming handlers
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ sessions.controller.ts  âŒ Session CRUD
â””â”€â”€ routes/
    â””â”€â”€ sessions.routes.ts      âŒ Session endpoints
```

---

## ğŸ¯ **To See the UI Right Now:**

1. **Run the frontend:**
   ```bash
   npm run dev
   ```

2. **Navigate to:**
   - http://localhost:5173/login
   - Login with demo credentials
   - Click "Start Session" on dashboard
   - See the beautiful UI! âœ…

3. **What you'll see:**
   - âœ… Gorgeous session lobby
   - âœ… Avatar selection
   - âœ… Voice selection
   - âœ… Duration picker
   - âœ… Live session interface
   - âœ… All buttons and controls
   - âœ… Smooth animations

---

## ğŸ¬ **What the Final Feature Will Look Like:**

### **User Flow:**
1. User clicks "Start Session" â†’ Session Lobby
2. User selects avatar & voice â†’ Click "Start Now"
3. **Permission request** â†’ Mic/Camera access
4. **Live video appears** â†’ User sees AI avatar
5. **User speaks** â†’ Whisper transcribes in real-time
6. **GPT-4 responds** â†’ AI generates contextual response
7. **ElevenLabs speaks** â†’ User hears lifelike voice
8. **Conversation continues** â†’ Natural back-and-forth
9. **Safety monitoring** â†’ Crisis detection in background
10. **User ends session** â†’ Credits deducted, history saved

---

## â±ï¸ **Implementation Time Estimate:**

| Task | Time | Priority |
|------|------|----------|
| Setup AI service | 2h | CRITICAL |
| Whisper integration | 2h | CRITICAL |
| ElevenLabs integration | 2h | CRITICAL |
| WebSocket setup | 3h | CRITICAL |
| Session management | 2h | HIGH |
| Frontend connection | 1h | HIGH |
| Testing & polish | 2h | MEDIUM |
| **TOTAL** | **14 hours** | - |

---

## ğŸ’¡ **Key Points:**

1. âœ… **Frontend is 100% complete** - Beautiful UI ready
2. âŒ **Backend is 0% complete** - Needs implementation
3. ğŸ¯ **Follow Task 7.1-7.13** in the checklist
4. â±ï¸ **~14 hours of work** to make it fully functional
5. ğŸ”‘ **You'll need:**
   - OpenAI API key (for Whisper + GPT-4)
   - ElevenLabs API key (for voice synthesis)
   - WebSocket server (Socket.io)
   - Supabase database (for sessions)

---

## ğŸš€ **Next Steps:**

1. **See the UI now**: http://localhost:5173/app/session-lobby
2. **Start backend**: Follow `/backend-starter/BACKEND_TASKS_CHECKLIST.md`
3. **Phase 7 tasks**: Tasks 7.1 through 7.13
4. **Get API keys**: OpenAI + ElevenLabs
5. **Implement WebSocket**: Real-time audio streaming
6. **Connect frontend**: Update ActiveSession.tsx

---

**Bottom Line**: The voice AI feature has an **amazing, production-ready UI** but needs the **backend services implemented** to make it actually work with real AI voices! ğŸ™ï¸âœ¨
